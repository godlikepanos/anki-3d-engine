// Copyright (C) 2009-present, Panagiotis Christopoulos Charitos and contributors.
// All rights reserved.
// Code licensed under the BSD License.
// http://www.anki3d.org/LICENSE

#pragma anki mutator HZB_TEST 0 1
#pragma anki mutator DISTANCE_TEST 0 1
#pragma anki mutator GATHER_AABBS 0 1
#pragma anki mutator HASH_VISIBLES 0 1
#pragma anki mutator GATHER_MESHLETS 0 1
#pragma anki mutator GATHER_LEGACY 0 1

#pragma anki skip_mutation DISTANCE_TEST 1 HZB_TEST 1
#pragma anki skip_mutation GATHER_MESHLETS 0 GATHER_LEGACY 0

#pragma anki technique comp

#include <AnKi/Shaders/Common.hlsl>
#include <AnKi/Shaders/Include/GpuSceneTypes.h>
#include <AnKi/Shaders/Include/GpuVisibilityTypes.h>
#include <AnKi/Shaders/VisibilityAndCollisionFunctions.hlsl>

#define NUMTHREADS 64

// Buffers that point to the GPU scene
StructuredBuffer<GpuSceneRenderableBoundingVolume> g_renderableBoundingVolumes : register(t0);
StructuredBuffer<GpuSceneRenderable> g_renderables : register(t1);
StructuredBuffer<GpuSceneMeshLod> g_meshLods : register(t2);
StructuredBuffer<Mat3x4> g_transforms : register(t3);

// 1st counter is the visible renderable count, 2nd the visible meshlet count and 3rd the number of threadgroups having been executed
RWStructuredBuffer<U32> g_counters : register(u0);

#if GATHER_LEGACY
RWStructuredBuffer<GpuVisibilityVisibleRenderableDesc> g_visibleRenderables : register(u1); // Indices of visible renderables
#endif
#if GATHER_MESHLETS
RWStructuredBuffer<GpuVisibilityVisibleMeshletDesc> g_visibleMeshlets : register(u2); // Descriptors of visible meshlets
#endif

// One U32 per bucket. For each bucket it's the offset where stage 2 will put the per-bucket results
#if GATHER_LEGACY
RWStructuredBuffer<U32> g_renderablePrefixSums : register(u3);
#endif
#if GATHER_MESHLETS
RWStructuredBuffer<U32> g_meshletPrefixSums : register(u4);
#endif

RWStructuredBuffer<DispatchIndirectArgs> g_gpuVisIndirectDispatchArgs : register(u5); // 2 elements. One for MDI and another for meshlets

RWStructuredBuffer<U32> g_outOfMemoryBuffer : register(u6);

#if GATHER_AABBS
RWStructuredBuffer<U32> g_visibleAabbIndices : register(u7); // Indices of the visible AABBs. The 1st element is the count.
#endif

#if HASH_VISIBLES
RWStructuredBuffer<GpuVisibilityHash> g_hash : register(u8);
#endif

#if DISTANCE_TEST == 0
ConstantBuffer<FrustumGpuVisibilityConsts> g_consts : register(b0);
#else
ANKI_FAST_CONSTANTS(DistanceGpuVisibilityConstants, g_consts)
#endif

#if HZB_TEST
Texture2D<Vec4> g_hzbTex : register(t4);
SamplerState g_nearestAnyClampSampler : register(s0);
#endif

Bool isVisible(GpuSceneRenderableBoundingVolume bvolume)
{
	const Vec3 sphereCenter = (bvolume.m_aabbMin + bvolume.m_aabbMax) * 0.5f;
	const F32 sphereRadius = bvolume.m_sphereRadius;

#if DISTANCE_TEST == 0
	// Frustum test
	//
	if(!frustumTest(g_consts.m_clipPlanes, sphereCenter, sphereRadius))
	{
		return false;
	}

	// Screen-space AABB calculation and checking
	//
	Vec2 minNdc, maxNdc;
	F32 aabbMinDepth;
	projectAabb(bvolume.m_aabbMin, bvolume.m_aabbMax, g_consts.m_viewProjectionMat, minNdc, maxNdc, aabbMinDepth);

	if(any(minNdc > 1.0f) || any(maxNdc < -1.0f))
	{
		// Outside of the screen
		return false;
	}

	const Vec2 windowCoordsMin = ndcToUv(minNdc) * g_consts.m_finalRenderTargetSize;
	const Vec2 windowCoordsMax = ndcToUv(maxNdc) * g_consts.m_finalRenderTargetSize;
	if(any(round(windowCoordsMin) == round(windowCoordsMax)))
	{
		// Doesn't touch the sampling points
		return false;
	}

	// HiZ culling
	//
#	if HZB_TEST
	if(cullHzb(minNdc, maxNdc, aabbMinDepth, g_hzbTex, g_nearestAnyClampSampler))
	{
		return false;
	}
#	endif // HZB_TEST

#else // DISTANCE_TEST == 1
	if(!testSphereSphereCollision(sphereCenter, sphereRadius, g_consts.m_pointOfTest, g_consts.m_testRadius))
	{
		return false;
	}
#endif

	return true;
}

[numthreads(NUMTHREADS, 1, 1)] void main(UVec3 svDispatchThreadId : SV_DISPATCHTHREADID, U32 svGroupIndex : SV_GROUPINDEX)
{
	const U32 bvolumeIdx = svDispatchThreadId.x;
	const U32 bvolumeCount = getStructuredBufferElementCount(g_renderableBoundingVolumes);
	Bool skip = (bvolumeIdx >= bvolumeCount);

	GpuSceneRenderableBoundingVolume bvolume;
	if(!skip)
	{
		bvolume = SBUFF(g_renderableBoundingVolumes, bvolumeIdx);
		skip = !isVisible(bvolume);
	}

#if GATHER_MESHLETS
	const U32 maxVisibleMeshlets = getStructuredBufferElementCount(g_visibleMeshlets);
#endif
#if GATHER_LEGACY
	const U32 maxVisibleInstances = getStructuredBufferElementCount(g_visibleRenderables);
#endif

#if GATHER_LEGACY && !GATHER_MESHLETS
	const U32 bucketCount = getStructuredBufferElementCount(g_renderablePrefixSums);
#elif GATHER_MESHLETS && !GATHER_LEGACY
	const U32 bucketCount = getStructuredBufferElementCount(g_meshletPrefixSums);
#else
	const U32 bucketCount = getStructuredBufferElementCount(g_meshletPrefixSums);
#endif

	if(!skip)
	{
		// Object is visible, add it to a bunch of buffers

		// Compute the LOD
		//
		const Vec3 sphereCenter = (bvolume.m_aabbMin + bvolume.m_aabbMax) * 0.5f;
		const F32 sphereRadius = bvolume.m_sphereRadius;
		const F32 distFromLodPoint = length(sphereCenter - g_consts.m_lodReferencePoint) - sphereRadius;

		U32 lod;
		if(distFromLodPoint < g_consts.m_maxLodDistances[0])
		{
			lod = 0u;
		}
		else if(distFromLodPoint < g_consts.m_maxLodDistances[1])
		{
			lod = 1u;
		}
		else
		{
			lod = 2u;
		}

		// Add the object
		//
		const U32 renderableIdx = bvolume.m_renderableIndex_20bit_renderStateBucket_12bit >> 12u;
		const GpuSceneRenderable renderable = SBUFF(g_renderables, renderableIdx);
		const U32 renderStateBucket = bvolume.m_renderableIndex_20bit_renderStateBucket_12bit & ((1u << 12u) - 1u);
		const U32 meshLodIndex = renderable.m_meshLodsIndex + lod;
		const GpuSceneMeshLod meshLod = SBUFF(g_meshLods, meshLodIndex);

		const Bool isParticleEmitter = renderable.m_particleEmitterIndex < kMaxU32 || renderable.m_particleEmitterIndex2 < kMaxU32;
		ANKI_MAYBE_UNUSED(isParticleEmitter);

		const Bool hasMeshlets = meshLod.m_meshletCount != 0u;
		if(hasMeshlets)
		{
#if GATHER_MESHLETS
			GpuVisibilityVisibleMeshletDesc desc;
			desc.m_renderableIndex = renderableIdx;
			desc.m_renderStateBucket = renderStateBucket;
			desc.m_lod = lod;
			desc.m_meshletIndex = 0;

			// X dimension will be fixed later
			U32 firstMeshletIndex;
			InterlockedAdd(SBUFF(g_counters, (U32)GpuVisibilityCounter::kMeshletsSurvivingStage1Count), meshLod.m_meshletCount, firstMeshletIndex);

			if(firstMeshletIndex + meshLod.m_meshletCount > maxVisibleMeshlets)
			{
				// OoM, do nothing
			}
			else
			{
				// All good

				// Store the meshlet descriptors
				for(U32 i = 0; i < meshLod.m_meshletCount; ++i)
				{
					SBUFF(g_visibleMeshlets, firstMeshletIndex + i) = desc;
					++desc.m_meshletIndex;
				}

				// Add to the next bucket
				if(renderStateBucket + 1u < bucketCount)
				{
					InterlockedAdd(SBUFF(g_meshletPrefixSums, renderStateBucket + 1u), meshLod.m_meshletCount);
				}
			}
#endif
		}
		else
		{
#if GATHER_LEGACY
			// X dimension will be fixed later
			U32 firstInstance;
			InterlockedAdd(SBUFF(g_counters, (U32)GpuVisibilityCounter::kVisibleRenderableCount), 1, firstInstance);

			if(firstInstance >= maxVisibleInstances)
			{
				// OoM, do nothing
			}
			else
			{
				// All good

				// Store the renderable
				GpuVisibilityVisibleRenderableDesc visRenderable;
				visRenderable.m_lod = lod;
				visRenderable.m_renderableIndex = renderableIdx;
				visRenderable.m_renderStateBucket = renderStateBucket;
				SBUFF(g_visibleRenderables, firstInstance) = visRenderable;

				// Add to the next bucket
				if(renderStateBucket + 1u < bucketCount)
				{
					InterlockedAdd(SBUFF(g_renderablePrefixSums, renderStateBucket + 1u), 1u);
				}
			}
#endif
		}

#if HASH_VISIBLES
		// Update the renderables hash
		{
			// Transform a random point as a way to get a feel for the transform
			const Mat3x4 trf = SBUFF(g_transforms, renderable.m_worldTransformsIndex);
			const Vec3 pt = mul(trf, Vec4(1503.98f, 2006.8f, -1400.16f, 1.0f));
			const UVec3 ptu = UVec3(asuint(pt.x), asuint(pt.y), asuint(pt.z));

			U32 hash = ptu.x;
			hash ^= ptu.y;
			hash ^= ptu.z;
			hash ^= renderable.m_uuid;

			InterlockedXor(SBUFF(g_hash, 0).m_renderablesHash, hash);

			const Bool deformable = isParticleEmitter || renderable.m_boneTransformsOffset != 0;
			if(deformable)
			{
				SBUFF(g_hash, 0).m_containsDeformable = 1;
			}
		}
#endif

#if GATHER_AABBS
		U32 index;
		InterlockedAdd(SBUFF(g_visibleAabbIndices, 0), 1, index);
		SBUFF(g_visibleAabbIndices, index + 1) = bvolumeIdx;
#endif
	}

	// Sync to make sure all the atomic ops have finished before the following code reads them
	AllMemoryBarrierWithGroupSync();

	// Check if it's the last threadgroup running
	if(svGroupIndex == 0)
	{
		U32 threadgroupIdx;
		InterlockedAdd(SBUFF(g_counters, (U32)GpuVisibilityCounter::kThreadgroupCount), 1, threadgroupIdx);
		const U32 threadgroupCount = (bvolumeCount + NUMTHREADS - 1) / NUMTHREADS;
		const Bool lastThreadExecuting = (threadgroupIdx + 1 == threadgroupCount);

		if(lastThreadExecuting)
		{
			// Last thing executing, fixup some sizes

			DispatchIndirectArgs args;

			// Renderables
#if GATHER_LEGACY
			U32 visibleInstancesCount = SBUFF(g_counters, (U32)GpuVisibilityCounter::kVisibleRenderableCount);
			if(visibleInstancesCount > maxVisibleInstances)
			{
				// OoM, fix a few things and inform the CPU
				visibleInstancesCount = maxVisibleInstances;
				SBUFF(g_counters, (U32)GpuVisibilityCounter::kVisibleRenderableCount) = maxVisibleInstances;
				InterlockedOr(SBUFF(g_outOfMemoryBuffer, 0), 1);
			}

			args.m_threadGroupCountX = (visibleInstancesCount + NUMTHREADS - 1) / NUMTHREADS;
			args.m_threadGroupCountY = 1;
			args.m_threadGroupCountZ = 1;
			SBUFF(g_gpuVisIndirectDispatchArgs, (U32)GpuVisibilityIndirectDispatches::k2ndStageLegacy) = args;
#endif

			// Meshlets
#if GATHER_MESHLETS
			U32 meshletsForStage2Count = SBUFF(g_counters, (U32)GpuVisibilityCounter::kMeshletsSurvivingStage1Count);
			if(meshletsForStage2Count > maxVisibleMeshlets)
			{
				// OoM, fix a few things and inform the CPU
				meshletsForStage2Count = maxVisibleMeshlets;
				SBUFF(g_counters, (U32)GpuVisibilityCounter::kMeshletsSurvivingStage1Count) = maxVisibleMeshlets;
				InterlockedOr(SBUFF(g_outOfMemoryBuffer, 0), 1);
			}

			args.m_threadGroupCountX = (meshletsForStage2Count + NUMTHREADS - 1) / NUMTHREADS;
			args.m_threadGroupCountY = 1;
			args.m_threadGroupCountZ = 1;
			SBUFF(g_gpuVisIndirectDispatchArgs, (U32)GpuVisibilityIndirectDispatches::k2ndStageMeshlets) = args;
#endif

			// Prefix sums. Use atomics because nVidia flickers
			U32 prevLegacyVal = 0;
			U32 prevMeshletVal = 0;
			ANKI_MAYBE_UNUSED(prevMeshletVal);
			ANKI_MAYBE_UNUSED(prevLegacyVal);
			for(U32 i = 1; i < bucketCount; ++i)
			{
				U32 old;

#if GATHER_LEGACY
				// Equivalent to: g_renderablePrefixSums[i] += g_renderablePrefixSums[i - 1u]
				InterlockedAdd(SBUFF(g_renderablePrefixSums, i), prevLegacyVal, old);
				prevLegacyVal += old;
#endif

#if GATHER_MESHLETS
				// Equivalent to: g_meshletPrefixSums[i] += g_meshletPrefixSums[i - 1u]
				InterlockedAdd(SBUFF(g_meshletPrefixSums, i), prevMeshletVal, old);
				prevMeshletVal += old;
#endif
			}

			// Reset it for the next job
			SBUFF(g_counters, (U32)GpuVisibilityCounter::kThreadgroupCount) = 0;
		}
	}
}
